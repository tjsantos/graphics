<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Ray Tracer</title>
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">
  <link rel="stylesheet" href="../modern-normalize.css">
  <link rel="stylesheet" href="../style.css">
</head>

<body>
<main class="container">
<header class="text-center ungutter">
  <h1>Ray Tracer</h1>
  <img
    src="./images/ray-tracer-bunny-logo.jpg"
    alt="Ray traced bunny in Cornell box with direct illumination progressive global illumination"
  >
</header>
<h2 class="text-center">Overview</h2>
<p>
  This project implements a physically-based renderer using the well-known technique
  of ray tracing. More specifically, it uses path tracing to implement global illumination
  for more realistic lighting in 3D renderings. Furthermore, this ray tracer is optimized for
  speed and quality by using a Bounding Volume Hierarchy (BVH) as an acceleration structure and
  by using adaptive sampling as a noise reduction technique. Overall, it can produce quality
  renderings of 3D scenes in reasonable amounts of time. Some sample renderings are presented below,
  as well as explanations of the techniques and optimizations to make it all work.
</p>

<h2 class="text-center">Part 1: Ray Generation and Intersection</h2>
<p>
  Ray tracing works by emulating the way light travels in the real world--as photons originating
  from light sources and bouncing off various objects until it reaches your eyes. In practice,
  this is done in reverse, tracing the path light takes by starting from the eyes and looking
  in some direction at some object that reflects light from a light source. Thus, the ray
  tracer starts from a given camera position and view direction, generates appropriate viewing
  rays for each pixel in the image, and figure's out the pixel's color based on the object
  that the ray hit and its position/orientation relative to the scene's lighting.
</p>
<p>
  Let us first approach the ray generation. We start with an image that we want to create
  in screen space coordinates, i.e. 800 x 600 pixels from a particular viewpoint. Samples are
  generated for each pixel by generating rays from the camera to the image view plane pixel and
  transforming to world coordinates, giving us a parametric equation for the ray,
  <code>r(t) = o + td</code>,
  where <code>o</code> is where the ray originates, <code>d</code> is the direction the ray
  travels, and <code>t</code> is a parameter that varies to make <code>r(t)</code> represent
  any position on the ray.
</p>
<p>
  Next, we want to figure out which object the ray hits. We'll explore intersections with two
  types of geometries: triangles and spheres.
</p>
<p>
  We can figure out if the ray hits a triangle by calculating the barycentric coordinates of
  the intersection point with the plane containing the triangle. This can be done with the
  equation <code>o + td = (1 - u - v)p1 + u*p2 + v*p3</code>, where <code>p1, p2</code> and
  <code>p3</code> are the triangle points and <code>u, v, (1 - u - v)</code> are barycentric
  coordinates. This equation can be rearranged to find the unknown values (t, u, v)
  for the intersection:
</p>
<pre>
  td + u(p1 - p2) + v(p1 - p3) = p1 - o,
</pre>
<p>
  or in matrix-vector form,
</p>
<pre>
  [d, (p1 - p2), (p1 - p3)] * [t; u; v] = [p1 - o].
</pre>
<p>
  This can be solved with basic linear algebra techniques, but a particularly efficient approach
  is to use the
  <a href="https://en.wikipedia.org/wiki/M%C3%B6ller%E2%80%93Trumbore_intersection_algorithm" target="_blank">
    Möller–Trumbore intersection algorithm</a>.
  We can verify that the equation is solvable and has a valid <code>t > 0</code> and
  valid barycentric coordinates <code>u >= 0, v >= 0, (1 - u - v) >= 0</code> that land inside
  the triangle to determine triangle intersection, and we are done!
</p>
<p>
  Now for spheres. To determine intersection here, we can substitute the ray position into the
  sphere equation and solve for <code>t</code>.
</p>
<pre>
  |(o + td - c)|^2 - R^2 = 0
</pre>
<p>
  Here, <code>c</code> is the sphere center, <code>R</code> is the sphere radius, and
  <code>o + td</code> is a point on the sphere when the equation holds. Expanding this
  out, we get a quadratic equation for <code>t</code> (the "." here means dot product):
</p>
<pre>
  (d . d)t^2 + (2(o - c) . d)t + ((o - c) . (o - c) - R^2) = 0.
</pre>
<p>
  If we can solve for a valid <code>t > 0</code>, we have ray intersection with the sphere.
</p>
<p>
  Once we have the object intersection information, we can do a lighting and color calculation
  for the pixels to accurately represent what our eyes should see. For now, we can use a debug
  color based off the surface normals at the intersection to observe that our implementation of
  ray generation and object intersection works as expected.
</p>
<div class="image-grid-3 text-center">
  <div>
    <figure>
      <img src="./images/p1-CBspheres.png" alt="Ray traced spheres in a Cornell box.">
      <figcaption>
      </figcaption>
    </figure>
  </div>
  <div>
    <figure>
      <img src="./images/p1-cow.png" alt="Ray traced cow.">
      <figcaption>
      </figcaption>
    </figure>
  </div>
  <div>
    <figure>
      <img src="./images/p1-teapot.png" alt="Ray traced teapot.">
      <figcaption>
      </figcaption>
    </figure>
  </div>
</div>

<h2 class="text-center">Part 2: Bounding Volume Hierarchy</h2>
<p>
  Though we now have the methods to compute ray-object intersections,
  it would still take <code>O(n)</code> time to loop through all <code>n</code>
  objects and test them against the ray. This can be improved significantly to
  <code>O(log n)</code> time, which is a speedup by a factor of 100,000x or more
  in practice, depending on the scene. To do this, we can use what are typically
  called ray tracing acceleration structures, which are spatial data structures
  that help optimize ray traversal performance.
</p>
<p>
  Here, we will use a
  <a href="https://en.wikipedia.org/wiki/Bounding_volume_hierarchy">
    bounding volume hierarchy (BVH)</a>,
  which is a tree structure that partitions all the shapes in the scene into disjoint groups
  of objects, whose bounding boxes may overlap. The ray then only needs to traverse the
  subset of objects within the bounding boxes that it intersects.
</p>
<figure class="text-center">
  <img src="./images/p2-bvh.png" alt="Bounding volume hierarchy (BVH) visualization.">
  <figcaption>
    Bounding volume hierarchy (BVH) visualization. The entire cow is contained
    within the initial bounding box. The triangles on the cow's head are
    contained in a descendant, smaller bounding box.
  </figcaption>
</figure>
<p>
  There are several heuristics that one can choose to do the actual partitioning and
  construction of the BVH. The approach that is taken here is to split a parent
  bounding box into two children by the midpoint along the box's longest side.
  This would help partition space more evenly so that the ray can skip the traversal
  of more objects on average.
</p>
<p>
  The time to ray trace a 3D scene with 1 ray per pixel is measured in the table below,
  using both the naive approach (testing every object against the ray) and the
  BVH accelerated approach. As we can see, the BVH helps render scenes of even hundreds
  of thousands of triangles within a fraction of a second. Without the BVH, the same
  scenes can take several minutes for the same result. This difference would be magnified
  even more as we increase the resolution, the samples per pixel, and the number of objects
  in the scene.
</p>
<table class="text-center mx-auto border-spacing-4">
  <caption>
    Time to render various 3D scenes with and without BVH acceleration.
  </caption>
  <thead>
    <tr>
      <th>Scene</th>
      <th>Number of Triangles</th>
      <th>Naive Approach (seconds)</th>
      <th>BVH Accelerated (seconds)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Teapot</td>
      <td>2,464</td>
      <td>9.5083</td>
      <td>0.0417</td>
    </tr>
    <tr>
      <td>Cow</td>
      <td>5,856</td>
      <td>20.0684</td>
      <td>0.0480</td>
    </tr>
    <tr>
      <td>Bunny</td>
      <td>33,696</td>
      <td>99.6719</td>
      <td>0.0415</td>
    </tr>
    <tr>
      <td>Bench</td>
      <td>67,808</td>
      <td>130.8857</td>
      <td>0.0198</td>
    </tr>
    <tr>
      <td>Dragon</td>
      <td>105,120</td>
      <td>328.4802</td>
      <td>0.0478</td>
    </tr>
    <tr>
      <td>Wall-E</td>
      <td>240,326</td>
      <td>(over 10 minutes)</td>
      <td>0.0705</td>
    </tr>
  </tbody>
</table>
<p>
  Here are a few of the moderately complex scenes that can now be ray traced
  with BVH acceleration, using simple surface normal (debug) coloring and 1 ray sample per pixel.
  Without using an acceleration structure like the BVH, we would not even be able
  to render some 3D scenes at a decent level of quality and complexity.
</p>
<div class="image-grid-3 text-center">
  <div>
    <figure>
      <img src="./images/p2-bunny-cropped.png" alt="BVH accelerated ray traced bunny.">
      <figcaption>
      </figcaption>
    </figure>
  </div>
  <div>
    <figure>
      <img src="./images/p2-wall-e-cropped.png" alt="BVH accelerated ray traced Wall-E.">
      <figcaption>
      </figcaption>
    </figure>
  </div>
  <div>
    <figure>
      <img src="./images/p2-dragon-cropped.png" alt="BVH accelerated ray traced dragon.">
      <figcaption>
      </figcaption>
    </figure>
  </div>
</div>

<h2 class="text-center">Part 3: Direct Illumination</h2>

<h2 class="text-center">Part 4: Global Illumination</h2>

<h2 class="text-center">Part 5: Adaptive Sampling</h2>


</main>

</body>
</html>
